---
title: "The imposter syndrome is strong with this one"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  # cache = TRUE,
  message = FALSE,
  warning = FALSE
)
```

```{r}
library(tidyverse)
library(tidymodels)
library(stacks)
dir <- '20210302'
path_res_knn <- here::here(dir, 'res_knn.rds')
path_res_rf <- here::here(dir, 'res_rf.rds')
path_fit_ens <- here::here(dir, 'fit_ens.rds')
path_preds_holdout <- here::here(dir, 'preds_holdout_v2.csv')
path_probs_holdout <- here::here(dir, 'probs_holdout_v2.csv')
df <- 
  here::here(dir, 'sliced-s00e01-data.csv') %>% 
  read_csv(guess_max = 20000) %>% 
  select(-X1)

# df_holdout <-
#   here::here(dir, 'sliced-s00e01-holdout.csv') %>% 
#   read_csv()

# Had to download the actual data set to get the `match` column.
df_holdout <-
  file.path(dir, 'Speed Dating Data.csv') %>% 
  read_csv(guess_max = 20000) %>% 
  filter(wave == 11) %>% 
  select(one_of(names(df)))
df_holdout
```

```{r}
col_y <- 'match'
nms <- df %>% names()
cols_x <- nms %>% setdiff(col_y)
cols_x_paired <- 
  cols_x %>%
  str_remove('_o$') %>% 
  tibble(col = .) %>% 
  count(col) %>% 
  filter(n > 1L) %>% 
  filter(col != 'dec')
cols_x_paired

f_select <- function(data) {
  res <-
    data %>% 
    select(
    any_of(col_y),
    one_of(cols_x_paired %>% pull(col)),
    one_of(cols_x_paired %>% pull(col) %>% paste0('_o'))
  )
  
  if(any('match' %in% colnames(res))) {
    res <-
      res %>% 
      mutate(across(match, factor))
  }
  res
}

df_slim <- df %>% f_select()
df_holdout_slim <- df_holdout %>% f_select()
```

```{r}
df_slim %>% 
  skimr::skim()
rec <-
  recipe(match ~ ., data = df_slim) %>% 
  step_zv(all_numeric_predictors()) %>% 
  step_impute_knn(all_numeric_predictors()) %>% 
  # step_impute_knn(
  #   age, impute_with
  # )
  themis::step_downsample(match)

# Just checking that it works.
jui <-
  rec %>% 
  prep() %>% 
  juice()
jui %>% skimr::skim()

jui_holdout <-
  rec %>% 
  prep() %>% 
  bake(new_data = df_holdout_slim)
jui_holdout %>% skimr::skim()
```

```{r }
spec_knn <-
  nearest_neighbor(neighbors = tune(), weight_func = tune()) %>% 
  set_mode('classification') %>% 
  set_engine('kknn') 

wf_knn <- 
  workflow() %>% 
  add_recipe(rec) %>% 
  add_model(spec_knn)

spec_rf <-
  rand_forest(mtry = tune(), trees = tune()) %>% 
  set_mode('classification') %>% 
  set_engine('ranger')

wf_rf <-
  workflow() %>% 
  add_recipe(rec) %>% 
  add_model(spec_rf)

ctrl_grid <- control_grid(save_pred = TRUE, save_workflow = TRUE, verbose = TRUE)
met_set <- metric_set(mn_log_loss, accuracy, roc_auc)
```

```{r, eval=F}
set.seed(6669)
folds <- df_slim %>% vfold_cv(strata = match, v = 10)
grid_knn <-
  grid_max_entropy(
    wf_knn %>% parameters(),
    size = 10
  )
grid_knn

# I feel like this is overfitting
grid_rf <-
  grid_max_entropy(
    trees(),
    # finalize(mtry(), df_slim),
    mtry(range = c(1, round(7/8*ncol(df_slim)-1))),
    size = 10
  )
grid_rf

f_tune <- 
  partial(
    tune_grid,
    resamples = folds,
    metrics = met_set,
    control = ctrl_grid,
    ... = 
  )

res_knn <- wf_knn %>% f_tune(grid = grid_knn)
res_knn
res_rf <- wf_rf %>% f_tune(grid = grid_rf)
res_rf
```

```{r eval=F, include=F}
write_rds(res_knn, path_res_knn)
write_rds(res_rf, path_res_rf)
```

```{r include=F}
res_knn <- read_rds(path_res_knn)
res_rf <- read_rds(path_res_rf)
```

```{r}
params_best_rf <- res_rf %>% select_best(metric = 'mn_log_loss')
wf_rf_final <- wf_rf %>% finalize_workflow(params_best_rf)
wf_rf_final
```

Looking at how the tuning went.

```{r}
mets_knn <-
  res_knn %>% 
  collect_metrics()
params_best_knn <- res_knn %>% select_best('mn_log_loss')
wf_knn_best <-
  wf_knn %>% 
  finalize_workflow(params_best_knn)
fit_knn_best <-
  wf_knn_best %>% 
  fit(data = df_slim) %>% 
  pull_workflow_fit()
fit_knn_best
mets_knn %>%
  select(neighbors, weight_func, .metric, mean) %>% 
  ggplot() +
  aes(x = neighbors, y = mean) +
  geom_point(aes(color = weight_func), size = 4) +
  facet_wrap(~.metric, scales = 'free') +
  guides(color = guide_legend(override.aes = list(size = 4))) +
  theme_minimal(base_size = 16) +
  theme(legend.position = 'top')
```

```{r}
mets_rf <-
  res_rf %>% 
  collect_metrics()
mets_rf

mets_rf %>%
  # filter(.metric == 'mn_log_loss') %>% 
  #mutate(across(trees, factor)) %>% 
  ggplot() +
  aes(x = mtry, y = mean) +
  geom_point(aes(color = trees, size = trees), size = 4) +
  facet_wrap(~.metric, scales = 'free') +
  guides(color = guide_legend(override.aes = list(size = 4))) +
  theme_minimal(base_size = 16) +
  theme(legend.position = 'top')
```

Variable importance time. (This is where i realized i left in `dec` and `dec_o` on accident.)

```{r}
spec_rf_vi <-
  rand_forest(mtry = 12, trees = 1970) %>% 
  set_mode('classification') %>% 
  set_engine('ranger', importance = 'impurity')
wf_rf_vi <-
  workflow() %>% 
  add_recipe(rec) %>% 
  add_model(spec_rf_vi)
fit_rf_vi <- wf_rf_vi %>% fit(data = df_slim)
fit_rf_vi

fit_rf_final <-
  fit_rf_vi %>% 
  pull_workflow_fit() 
gg_vi <-
  fit_rf_vi %>% 
  pull_workflow_fit() %>% 
  vip::vip() +
  theme_minimal(base_size = 18) +
  labs(
    title = 'Variable Importance'
  )
gg_vi
```

Ensembling time with a package i just learned how to use yesterday! This is basically a regularized linear regression on our knn and rf models.

```{r }
ens_st <-
  stacks() %>% 
  add_candidates(res_knn) %>% 
  add_candidates(res_rf)
ens_st
```

```{r, eval=F}
fit_ens <-
  ens_st %>% 
  blend_predictions() %>% 
  fit_members()
fit_ens
```

A blend of 6 rf models were chosen by the ensembling

```{r, eval=F, include=F}
write_rds(fit_ens, path_fit_ens)
```

```{r, include=F}
fit_ens <- read_rds(path_fit_ens)
```

```{r, eval=F, include=F}
probs_ens <-
  fit_ens %>% 
  predict(new_data = df_slim, type = 'prob') %>% 
  rename(prob_0 = .pred_0, prob_1 = .pred_1) %>% 
  bind_cols(df_slim %>% select(match))
probs_ens

probs_ens %>% 
  mn_log_loss(truth = match, .pred_0)
```

```{r}
probs_holdout <-
  fit_ens %>% 
  predict(
    # df_holdout_slim, 
    df_actual_slim,
    type = 'prob'
  ) %>% 
  rename(prob_0 = .pred_0, prob_1 = .pred_1) %>% 
  bind_cols(df_actual_slim %>% select(match))
probs_holdout

ll_holdout <-
  probs_holdout %>% 
  mn_log_loss(truth = match, prob_0)
ll_holdout
write_csv(probs_holdout, path_probs_holdout)
```
